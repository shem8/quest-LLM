id: quiz_llm_core_features
learningObjectives:
  - Test your understanding of the foundational features of LLMs.
hints:
  - Think about how the model assigns importance to different parts of the input data.
  - "Chapter 3 of 'Hands-On Large Language Models' explores the Transformer architecture in detail"
startFlow:
  do:
    - actionId: bot_message
      params:
        person: lucca
        messages:
          - text: Now let's put our knowledge to the test with a quiz.
          - text: We've covered a lot of ground - from understanding what LLMs are to diving deep into the Transformer architecture. As Chapter 3 of 'Hands-On Large Language Models' shows us, the self-attention mechanism is truly the heart of these powerful models.
          - text: So here's your challenge - How does the self-attention mechanism within the Transformer architecture contribute to its performance?
    - actionId: quiz_message
      name: quiz
      params:
        person: lucca
        options:
          - It helps the model focus on relevant parts of the input sentence.
          - It enables the model to process data sequentially.
          - It increases the complexity of the model exponentially.
trigger:
  type: chat_form_submitted
  flowNode:
    switch:
      key: ${formSubmission}
      cases:
        A:
          do:
            - actionId: bot_message
              params:
                person: lucca
                messages:
                  - text: Exactly! The self-attention mechanism is like giving the model selective focus - it can weigh the importance of different words in a sentence for better understanding.
            - actionId: finish_step
        B:
          do:
            - actionId: bot_message
              params:
                person: lucca
                messages:
                  - text: Not quite. Sequential processing is not the main strength of self-attention.
            - actionId: replay_action
              params:
                actionName: quiz
        C:
          do:
            - actionId: bot_message
              params:
                person: lucca
                messages:
                  - text: Well, not necessarily. Self-attention uses fixed network sizes to balance complexity.
            - actionId: replay_action
              params:
                actionName: quiz
